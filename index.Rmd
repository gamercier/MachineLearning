---
title: "Machine Learning - Qualitative Activity"
author: "Gustavo Mercier"
date: "December 26, 2015"
output:
  html_document:
    hightlight: tango
    keep_md: yes
    theme: readable
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, fig.width=6, fig.height=8, fig.align="center")
```

##Summary
It is possible to detect specific human activities by the analysis of data generated by sensors sensitive to position and acceleration. However, detecting how good a specific activity conforms to a preset standard, i.e. detecting quality of the motion, is a new area of research. Here we present a machine learning approach to the problem.

Using data from Velloso et al. (Velloso et al. *Qualitative Activity Recognition of Weight Lifting Exercises*, Proceedings of the 4th International Conference in Cooperation with SIGCHI [Augmented Human '13], Stuggart, Germany: ACM SIGCHI, 2013), we develop a tree classifier capable of discriminating between a single arm dumbbell lift exercise done with good form or done with one of four common mistakes. The tree achieves an overall accuracy of 83% (95% CI: 82.1% - 84.2%) on cross validation. However, when confronted with a true unknown the accuracy was 75%. Velloso et. al. achieved better accuracy, but applied a more complex model using not just raw data but computed features in a random forest with bagging, (97.3% to 99.1% depending on metric).

##Introduction
Human wearable devices that contain accelerometers, gyroscopes, and magnetometers are inertial measurement units (IMU) that can generate real time data encoding the motion of the individual wearing the device. This provides a unique opportunity for real time interactions between the device and the human, and is the motivation behind such devices like the Apple iWatch.

There is a need to not just detect activity, but to detect the quality of this activity, i.e. does it conform to some standard, and to what degree it deviates from the standard. This need stems from an interest of providing realtime feedback such as in exercising. To do this it is necessary to use the input data from the IMU to predict the activty, and the quality of the activity of the wearer. Machine learning algorithms are ideally suited for this task, and here we illustrate the development of one such algorithm using data from a one arm dumbbell weight lifting exercise.

##Data
The raw data is supplied by Velloso et al. *Qualitative Activity Recognition of Weight Lifting Exercises*, Proceedings of the 4th International Conference in Cooperation with SIGCHI [Augmented Human '13], Stuggart, Germany: ACM SIGCHI, 2013. It consists of a *csv* formatted file containing measurements of the IMU (cartesian vectors for acceleration in units of "g", angular velocity in "rads/sec", and magnetic field in "microteslas", as well as total acceleration and the three Euler angles - roll, yaw, and pitch). These measurements we label the *raw numbers*. The data also includes an incomplete list of features computed from the raw numbers.

The raw numbers are recorded at a frequency of 45Hz for each one of 6 individuals who wear devices in the waist (belt), arm, forearm, and in the dumbbell. Each individual makes 10 repetitions of the one arm dumbbell (weight 1.25 kg) lift while supervised by a trainer who instructs the subject to do the lift in 5 different ways: good form (classe: A), throwing the elbows to the front (classe B), lifting the dumbbell halfway (classe C), lowering the dumbbell only halfway (classe D), and throwing the hips to the front (classe E). The total raw dataset has dimensions 19622 X 160.

###Cleaning the data

The final exercise for this course provides input predictors that are based on the raw numbers, and not computed features. For this reason we focus on the raw numbers in the training data set (pml-training.csv), and generate a machine that uses the 52 raw numbers to predict the response. The responses consist of one of 5 factors under column *classe* in our dataframe. This dataframe is labeled, *clean_data*. It has dimensions 19622 X 53.

```{r message=FALSE}
# Load raw data for training
setwd('~/git/MachineLearning')
data <- read.csv("./pml-training.csv")

templates <- c('roll_LOC', 'pitch_LOC','yaw_LOC','total_accel_LOC',
'gyros_LOC_x','gyros_LOC_y','gyros_LOC_z','accel_LOC_x','accel_LOC_y',
'accel_LOC_z','magnet_LOC_x','magnet_LOC_y','magnet_LOC_z')
metrics_kept <- c(gsub('LOC','belt',templates),
             gsub('LOC','arm',templates),
             gsub('LOC','dumbbell',templates),
             gsub('LOC','forearm',templates))

clean_data <- subset(data,select=c('classe',metrics_kept))
```
```{r}
str(clean_data,list.len=10)
```


###Building the machine

####Training and testing sets
Starting from the *clean_data* dataframe we set a seed and use the *caret* package to partition the data into *training* and *testing* dataframes. Approximately 75% of the data belongs to the training set.

```{r message=FALSE}
# split data in training and validation sets
library(caret)
set.seed(6547)
inTrain = createDataPartition(clean_data$classe,p = 3/4)[[1]]
training = clean_data[ inTrain,]
testing =  clean_data[-inTrain,]
d_training = dim(training)
d_testing = dim(testing)
```
```{r}
print(paste("Dimensions: Training=",d_training[1],"x",d_training[2],"Testing=", d_testing[1],"x",d_testing[2]))
```


####Choice of Model
Velloso et al. generated a very successful random forest (with bagging) model based on raw numbers and features computed from the data (i.e. their final machine had 17 features that included such variables as the mean of the roll at the waist). However, this machine is not useful for our current exercise.

This is because the final test for our model consists of single time point raw numbers as predictors and not computed features. In addition, creation of a random forest is out of the scope of the computer resources available for this project. So, what are our the options?

*Simple data exploration*

```{r echo=FALSE, message=FALSE}
cmx <- cor(subset(training,select=c(-classe)))
ex_cor <- cmx[1,9]
```

The predictors are highly correlated. For example, the correlation between roll_belt and accel_belt_y is `r round(ex_cor,3)`. This means that a simple regression model is not suitable, unless we preprocess the data to eliminate the correlations. Moreover, the need to work with single point raw numbers as predictors means that a model based on kinematics (i.e. the physics of motion) is not appropriate for our case.

```{r echo=FALSE, message=FALSE}
classes <- levels(training$classe)
by_classe <- list()
for(cl in classes) {
    by_classe[[cl]] <- subset(training,classe==cl)
    by_classe[[cl]] <- subset(by_classe[[cl]],select=-classe)
}
summaries <- lapply(by_classe,summary)
#Looking at means, standard deviations stratified by classe and metric
means <- lapply(by_classe,colMeans)
stdevs <- lapply(by_classe,function(x){sapply(x,sd)})
y_means <- c()
for(col in names(means)) y_means <- c(y_means,as.vector(means[[col]]))
y_stdevs <- c()
for(col in names(stdevs)) y_stdevs <- c(y_stdevs,as.vector(stdevs[[col]]))
x_classe <- c()
for(col in names(means)) x_classe <- c(x_classe,
                    rep(col,length(as.vector(means[[col]]))))
metric_type <- function(s){
    if(grepl("accel",s)) {
        return("Acceleration")
    }
    else if(grepl("gyros",s)) {
        return("Angular.Velocity")
    }
    else if(grepl("magnet",s)) {
        return("Magnet.Field")
    }
    else if(grepl("roll",s) || grepl("pitch",s) || grepl("yaw",s)){
        return("Euler")
    }
    return("Other")
}
x_metric <- c()
for(col in names(means)){
    x_metric <- c(x_metric,
                  as.vector(sapply(names(by_classe[[col]]),metric_type))
    )
}
to_plot <- data.frame(classe=x_classe,means=y_means,stdevs=y_stdevs,
                      metric=x_metric)
```
```{r message=FALSE}
var_plot <- ggplot(data=to_plot,aes(classe,means)) +
    geom_point(aes(colour=metric,size=10*stdevs),alpha=0.5) +
    scale_colour_brewer(palette="Spectral") + geom_hline(aes(yintercept=160))
print(var_plot)
```
*Figure 1*

A display of the data as in Figure 1. where we plot the mean of the raw numbers color coded by class (i.e. acceleration terms, gyroscopic terms, magnetic terms, euler angles) and size of the dots coding standard deviations (multiplied by a factor of 10) illustrates one possible way of segregating between the different classes. The horizontal line is at y-intercept=160 and we can see that for classe A and C one of the acceleration terms has a mean higher than this value while the other three classes has means lower than this value. A collection of properly placed lines may prove a suitable classifier.


A tree is the classifier suggested by Figure 1. It is a simple machine, computationally accessible with our resources, and still well suited for our case. It is also an initial step into more complex classifers that require more computer power, such as random forests. A tree is immune to non-linearities in the relationship between our predictors and responses. However, because in our test case the predictors are not average values we work with raw numbers and not the means displayed in Figure 1.


###Construction of the Tree
Our tree is based on the *Recursive Partioning and Regression Tree* implemented throught the *rpart* parckage in the *caret* package. In constructing the tree we used all 52 predictors and a 10-fold cross validation scheme with three repetitions as recommended by [Kuhn's tutorial](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/). David Austin has an [essay](http://www.ams.org/samplings/feature-column/fc-2014-12) in the construction of trees that is simple enough for most readers to follow.

The cross validation scheme reduces the variance, a problem associated with trees. To improve the accuracy of the tree we increased the default *tuneLength* from 3 to 30. This effectively increased the tree depth by testing a larger number of smaller complexity parameters. However, it makes the tree more accurate at the risk of overfitting.

We look for a compromise between tree size and accuracy in the prediction. The cross validation allows us to prune the maximum tree (which overfitts the training set) for a given value fo the complexity parameter. The larger the complexity parameter the simpler but more inaccurate the tree will be. Our choice of complexity parameter is then an trade-off between tree size (and risk of overfitting) vs. accuracy.

```{r message=FALSE}
tcontrol <- trainControl(method='repeatedcv',repeats=3)
mRPART <- train(classe ~ ., method='rpart',data=training,tuneLength=30,
                trControl=tcontrol)
print(mRPART)
```

As shown above the best tree has a low complexity parameter, Cp=0.00303. At this level of Cp, the cross validation yields an estimated accuracy with a mean of 83.6% and a standard deviation of with a mean of 0.8%. Figure 2 below shows the progress in accuracy as the complexity parameter drops in the creation of the tree:

```{r message=FALSE}
results <- data.frame(mRPART$results)
results$split <- seq(30,1)
par(mar=c(5,4,4,5)+.1)
plot(results$split,results$cp,pch=19,bg="red",
     col="red",ylab="CP",xlab="Tree Split",ylim=c(0.00,0.15))
par(new=TRUE)
plot(results$split,results$Accuracy,pch=19,bg="blue",
     col="blue",xaxt="n",yaxt="n",xlab="",ylab="",ylim=c(0.00,1.00))
axis(4)
mtext("Accuracy(From Cross Validation)",side=4,line=3)
legend("topleft",col=c("red","blue"),pch=19,legend=c("CP","Accuracy"))
```
*Figure 2*

It is clear that for the best complexity parameter the accuracy has already leveled off. Further decreases in complexity parameter (steps not shown, but tested by increasing tuneLength from 30 to 40) lead to only modest increases in accuracy at the expense of much larger trees.


###Lessons from the Tree
One benefit of trees is to detect relevant variables that may be used in creating other models.

```{r message=FALSE}
print(varImp(mRPART))
```

The output above shows that roll_belt, pitch_forearm, magnet_dumbbell_z, accel_dumbbell_y, pitch_belt, and roll_forearm are in the top 25th percentile. **This implies that the position of the waist is a significant parameter to detect poor form in lifting the dumbbell**. Surprisingly, acceleration, a variable that is central in kinematic models, is less important.

Figure 3 is a contour plot of the normalized density histogram colored by class. The histograms describe the distribution for the roll_belt and pitch_forearm euler angles. Using these two angles the classes begin to segregate into groups. In the lower left corner, there is a subgroup that belongs entirely to the A class. In the upper right corner, a subgroup belongs to the D class. It is also clear that members of these classes extend beyond these two corners, and have significant overlap with other classes. It is not perfect but it shows how a tree can work.

```{r message=FALSE}
p <- ggplot(data=training,mapping=aes(roll_belt,pitch_forearm)) + geom_density_2d(aes(colour=classe))
print(p)
```
*Figure 3*

###Evaluation of the Prediction Model
The tree was tested against the *testing* data with the following results reported through a confusion matrix.

```{r message=FALSE}
oRPART <- predict(mRPART,testing)
cmRPART <- confusionMatrix(oRPART,testing$classe)
print(cmRPART)
```
The overall accuracy is 83% (95% CI: 82.1% - 84.2%). Positive predictive values for the different classes of responses range from as high as 89% for class A (good form) to as low as 77% for class C. Negative predictive values are higher in the mid 90%. The accuracy seen in the test data is in line with that predicted using the cross validation method, and reported by the *train* function in *caret*.


#Results against unknown cases
The performance of the tree against the unknown cases is slightly worse than predicted from the validation (testing) data set, 75%. A total of 5 cases were predicted incorrectly. Three of the five cases were class C, the one response for which the tree had the lowest positive predictive value.

This performance is below that predicted by cross validation and the confusion matrix result using the test set. One reason for the discrepancy is the underlying distribution of cases across these data sets. For example, if the unknown cases are all of classe C, then our confusion matrix shows that the positive predictive value will be closer to 77%!


#Conclusion:
We describe the construction of a tree capable of predicting the quality of a dumbbell lift exercise using raw numbers collected by IMU. A combination of cross validation and testing with unknown data shows that the accuracy is moderate in the upper 70 to lower 80%. A lot depends on the underlying distribution of the different classes that make up the training sets vs. the corresponding distribution in the unknown cases.


#Appendix:
[Final Tree](TreeFinal.pdf)

